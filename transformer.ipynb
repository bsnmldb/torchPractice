{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5194, 0.4806, 0.0000, 0.0000],\n",
      "         [0.3737, 0.6263, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3165, 0.3582, 0.3253, 0.0000],\n",
      "         [0.2171, 0.3787, 0.4042, 0.0000]]])\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4159, 0.3399, 0.2442, 0.0000]],\n",
      "\n",
      "        [[0.4289, 0.5711, 0.0000, 0.0000],\n",
      "         [0.2249, 0.2022, 0.3421, 0.2309]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntensor([[[1.0000, 0.0000, 0.0000, 0.0000],\\n         [0.4125, 0.3273, 0.2602, 0.0000]],\\n        [[0.5254, 0.4746, 0.0000, 0.0000],\\n         [0.3117, 0.2130, 0.1801, 0.2952]]])\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ScaledDotAttention(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, valid_lens=None):\n",
    "        \"\"\"\n",
    "        @param q: shape = (batch_size, query_num, hidden_size)\n",
    "        @param k, v: shape = (batch_size, kv_num, hidden_size)\n",
    "        @param valid_lens: shape = (batch_size,) or (batch_size, query_num)\n",
    "        @return: shape = (batch_size, query_num, hidden_size)\n",
    "        \"\"\"\n",
    "        hidden_size = q.shape[-1]\n",
    "        # scores: shape = (batch_size, query_num, kv_num)\n",
    "        scores = torch.bmm(q, k.transpose(1, 2)) / math.sqrt(hidden_size)\n",
    "        attention_weights = ScaledDotAttention.masked_softmax(scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(attention_weights), v)\n",
    "\n",
    "    @staticmethod\n",
    "    def masked_softmax(scores, valid_lens=None):\n",
    "        \"\"\"\n",
    "        @param scores: shape = (batch_size, query_num, kv_num)\n",
    "        @param valid_lens: shape = (batch_size,) or (batch_size, query_num)\n",
    "        @return: shape = (batch_size, query_num, kv_num)\n",
    "        \"\"\"\n",
    "        if valid_lens is not None:\n",
    "            if valid_lens.dim() == 1:\n",
    "                mask = torch.arange(scores.shape[-1])[None, :] >= valid_lens[:, None]\n",
    "                mask = torch.repeat_interleave(mask.unsqueeze(1), scores.shape[1], dim=1)\n",
    "                # mask = mask.unsqueeze(1).repeat(1, scores.shape[1], 1)\n",
    "            else:\n",
    "                mask = torch.arange(scores.shape[-1])[None, None, :] >= valid_lens[:, :, None]\n",
    "            scores[mask] = -torch.inf\n",
    "        return F.softmax(scores, dim=-1)\n",
    "\n",
    "x = ScaledDotAttention.masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))\n",
    "print(x)\n",
    "x = ScaledDotAttention.masked_softmax(torch.rand(2, 2, 4), torch.tensor([[1, 3], [2, 4]]))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, query_size, key_size, value_size, hidden_size, output_size, num_heads, dropout):\n",
    "        self.num_heads = num_heads\n",
    "        self.wq = nn.Linear(query_size, hidden_size, bias=False)\n",
    "        self.wk = nn.Linear(key_size, hidden_size, bias=False)\n",
    "        self.wv = nn.Linear(value_size, hidden_size, bias=False)\n",
    "        self.scaledDotAttention = ScaledDotAttention(dropout)\n",
    "        self.dense = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, q, k, v, valid_lens=None):\n",
    "        \"\"\"\n",
    "        @param q: shape = (batch_size, query_num, query_size)\n",
    "        @param k: shape = (batch_size, kv_num, key_size)\n",
    "        @param v: shape = (batch_size, kv_num, value_size)\n",
    "        @param valid_lens: shape = (batch_size,) or (batch_size, query_num)\n",
    "        @return: shape = (batch_size, query_num, hidden_size)\n",
    "        \"\"\"\n",
    "        q, k, v = map(self.transpose_qkv, (self.wq(q), self.wk(k), self.wv(v)))\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, self.num_heads, dim=0)\n",
    "        cat_hidden = self.scaledDotAttention(q, k, v, valid_lens)\n",
    "        return self.dense(self.invtranspose_qkv(cat_hidden))\n",
    "    \n",
    "    def transpose_qkv(self, tensor):\n",
    "        \"\"\"\n",
    "        @param tensor: shape = (batch_size, num, hidden_size)\n",
    "        @return: shape = (batch_size * num_heads, num, hidden_size / num_heads)\n",
    "        \"\"\"\n",
    "        tensor = tensor.reshape(tensor.shape[0], tensor.shape[1], self.num_heads, -1)\n",
    "        tensor = tensor.permute(0, 2, 1, 3)\n",
    "        return tensor.reshape(-1, tensor.shape[2], tensor.shape[3])\n",
    "\n",
    "    def invtranspose_qkv(self, tensor):\n",
    "        \"\"\"\n",
    "        @param tensor: shape = (batch_size * num_heads, num, hidden_size / num_heads)\n",
    "        @return: shape = (batch_size, num, hidden_size)\n",
    "        \"\"\"\n",
    "        tensor = tensor.reshape(-1, self.num_heads, tensor.shape[1], tensor.shape[2])\n",
    "        tensor = tensor.permute(0, 2, 1, 3)\n",
    "        return tensor.reshape(tensor.shape[0], tensor.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, hidden_size, ffn_size):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.Linear(hidden_size, ffn_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dense2(self.relu(self.dense1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.alpha = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.beta\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout=0):\n",
    "        super().__init__()\n",
    "        # self.ln = nn.LayerNorm(hidden_size)\n",
    "        self.ln = LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, fx):\n",
    "        return self.ln(x + self.dropout(fx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, hidden_size, max_length=1024):\n",
    "        super().__init__()\n",
    "        self.P = torch.zeros((max_length, hidden_size))\n",
    "        x = torch.arange(max_length).reshape(-1, 1) /\\\n",
    "            torch.pow(torch.tensor(10000), torch.arange(0, hidden_size, 2) / hidden_size).reshape(1, -1)\n",
    "        self.P[:, 0::2] = torch.sin(x)\n",
    "        self.P[:, 1::2] = torch.cos(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_length = x.shape[1]\n",
    "        return x + self.P[:seq_length, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, ffn_size, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(\n",
    "            hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, num_heads, dropout)\n",
    "        self.norm1 = AddNorm(hidden_size, dropout)\n",
    "        self.ffn = FFN(hidden_size, ffn_size)\n",
    "        self.norm2 = AddNorm(hidden_size, dropout)\n",
    "\n",
    "    def forward(self, x, valid_lens):\n",
    "        x = self.norm1(x, self.self_attn(x, x, x, valid_lens))\n",
    "        return self.norm2(x, self.ffn(x))\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_encoder, num_heads, ffn_size, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position = PositionEmbedding(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layers = nn.ModuleList(\n",
    "            ((f\"encoder_{i}\", EncoderLayer(hidden_size, num_heads, ffn_size, dropout)) for i in range(num_encoder))\n",
    "        )\n",
    "\n",
    "    def forward(self, x, valid_lens):\n",
    "        # TODO why times sqrt(d_model)\n",
    "        x = self.dropout(self.position(self.embedding(x) * math.sqrt(self.hidden_size)))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, valid_lens)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, ffn_size, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(\n",
    "            hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, num_heads, dropout)\n",
    "        self.norm1 = AddNorm(hidden_size, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(\n",
    "            hidden_size, hidden_size, hidden_size, hidden_size, hidden_size, num_heads, dropout)\n",
    "        self.norm2 = AddNorm(hidden_size, dropout)\n",
    "        self.ffn = FFN(hidden_size, ffn_size)\n",
    "        self.norm3 = AddNorm(hidden_size, dropout)\n",
    "\n",
    "    def forward(self, x, self_kv, encoder_output, valid_lens, seq_lens):\n",
    "        x = self.norm1(x, self.self_attn(x, self_kv, self_kv, valid_lens))\n",
    "        x = self.norm2(x, self.cross_attn(x, encoder_output, encoder_output, seq_lens))\n",
    "        return self.norm3(x, self.ffn(x))\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_decoder, num_heads, ffn_size, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position = PositionEmbedding(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layers = nn.ModuleList(\n",
    "            (f\"decoder_{i}\", DecoderLayer(hidden_size, num_heads, ffn_size, dropout)) for i in range(num_decoder)\n",
    "        )\n",
    "        self.dense = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, valid_lens, encoder_output, seq_lens):\n",
    "        x = self.dropout(self.position(self.embedding(x) * math.sqrt(self.hidden_size)))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, encoder_output, valid_lens, seq_lens)\n",
    "        return self.dense(x)\n",
    "\n",
    "    def predict(self, x, encoder_output, seq_lens):\n",
    "        x = self.dropout(self.position(self.embedding(x) * math.sqrt(self.hidden_size)))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x[:, -1, :], x, encoder_output, None, seq_lens)\n",
    "        return self.dense(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, hidden_size, encoder_blk, decoder_blk, ffn_size, num_heads, src_vocab, tgt_vocab, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, hidden_size, encoder_blk, num_heads, ffn_size, dropout)\n",
    "        self.decoder = Decoder(tgt_vocab, hidden_size, decoder_blk, num_heads, ffn_size, dropout)\n",
    "\n",
    "    def encode(self, x, seq_lens):\n",
    "        return self.encoder(x, seq_lens)\n",
    "\n",
    "    def decode(self, x, valid_lens, encoder_output, seq_lens):\n",
    "        return self.decoder(x, valid_lens, encoder_output, seq_lens)\n",
    "\n",
    "    def forward(self, x, seq_lens, y, valid_lens):\n",
    "        enc_output = self.encode(x, seq_lens)\n",
    "        return self.decode(y, valid_lens, enc_output, seq_lens)\n",
    "\n",
    "    def predict(self, x, encoder_output, seq_lens):\n",
    "        return self.decoder.predic(x, encoder_output, seq_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, lr, epoch, dataLoader, print_every=100):\n",
    "    optim = torch.optim.SGD(model.params(), lr=lr)\n",
    "    cretira = nn.CrossEntropyLoss()\n",
    "    for ep in range(epoch):\n",
    "        print(f\"----- epoch {ep} -----\")\n",
    "        step_count = 0\n",
    "        for X, y, seq_lens in dataLoader:\n",
    "            optim.zero_grad()\n",
    "            valid_lens = torch.arange(1, y.shape[1] + 1).unsqueeze(0).repeat(y.shape[0], 1)\n",
    "            logits = model(X, seq_lens, y, valid_lens)\n",
    "            loss = cretira(y, logits)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            step_count += 1\n",
    "            if step_count % print_every == 0:\n",
    "                print(f\"epoch {ep}, step {step_count}, ppl = {torch.exp(loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataLoader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "ffn_size = 2048\n",
    "N = 6\n",
    "num_heads = 8\n",
    "dropout = 0.1\n",
    "vocab_size = 1000\n",
    "transformer = Transformer(hidden_size, N, N, ffn_size, num_heads, vocab_size, vocab_size, dropout)\n",
    "\n",
    "train(transformer, lr=1e-4, epoch=5, dataLoader=dataLoader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
